{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face token\n",
    "hf_token = os.getenv(\"hf_token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "import peft\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1Which one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1 C feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democracy The president leads the Head of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are educated guessHowever prediction must be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61219</th>\n",
       "      <td>አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61220</th>\n",
       "      <td>ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61221</th>\n",
       "      <td>አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61222</th>\n",
       "      <td>በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61223</th>\n",
       "      <td>ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61224 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      Civics Unit 1 Grade 11 & 12 hybrid 1Which one ...\n",
       "1      Civics Unit 1 Grade 11 & 12 hybrid 1 C feature...\n",
       "2      democracy The president leads the Head of the ...\n",
       "3      አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...\n",
       "4      are educated guessHowever prediction must be t...\n",
       "...                                                  ...\n",
       "61219  አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...\n",
       "61220  ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...\n",
       "61221  አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...\n",
       "61222  በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...\n",
       "61223  ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...\n",
       "\n",
       "[61224 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '../../merged.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1Which one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1 C feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democracy The president leads the Head of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are educated guessHowever prediction must be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61219</th>\n",
       "      <td>አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61220</th>\n",
       "      <td>ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61221</th>\n",
       "      <td>አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61222</th>\n",
       "      <td>በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61223</th>\n",
       "      <td>ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61224 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      Civics Unit 1 Grade 11 & 12 hybrid 1Which one ...\n",
       "1      Civics Unit 1 Grade 11 & 12 hybrid 1 C feature...\n",
       "2      democracy The president leads the Head of the ...\n",
       "3      አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...\n",
       "4      are educated guessHowever prediction must be t...\n",
       "...                                                  ...\n",
       "61219  አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...\n",
       "61220  ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...\n",
       "61221  አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...\n",
       "61222  በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...\n",
       "61223  ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...\n",
       "\n",
       "[61224 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=df[['Text']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2=dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# # Create a dictionary containing your Amharic text data\n",
    "# data_dict = {\"Text\": dataset['Text'].tolist()}\n",
    "\n",
    "# # Create a Dataset object\n",
    "# dataset_2 = Dataset.from_dict(data_dict)\n",
    "# dataset_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1Which one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1 C feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democracy The president leads the Head of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are educated guessHowever prediction must be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61219</th>\n",
       "      <td>አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61220</th>\n",
       "      <td>ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61221</th>\n",
       "      <td>አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61222</th>\n",
       "      <td>በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61223</th>\n",
       "      <td>ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61224 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      Civics Unit 1 Grade 11 & 12 hybrid 1Which one ...\n",
       "1      Civics Unit 1 Grade 11 & 12 hybrid 1 C feature...\n",
       "2      democracy The president leads the Head of the ...\n",
       "3      አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...\n",
       "4      are educated guessHowever prediction must be t...\n",
       "...                                                  ...\n",
       "61219  አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...\n",
       "61220  ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...\n",
       "61221  አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...\n",
       "61222  በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...\n",
       "61223  ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...\n",
       "\n",
       "[61224 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (1.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: 44081\n",
      "evaluation dataset shape: 4898\n",
      "Testing dataset shape: 12245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_val_data, test_data = train_test_split(dataset_2, test_size=0.20, random_state=42)\n",
    "train_data, evaluation_data = train_test_split(train_val_data, test_size=0.10, random_state=42)\n",
    "\n",
    "print('Training dataset shape:', len(train_data))\n",
    "print('evaluation dataset shape:', len(evaluation_data))\n",
    "print('Testing dataset shape:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47104</th>\n",
       "      <td>እንፈልጋለን ፤ አጨራረሱን እንፈልጋለን ኦባ የስኬታችን አንዱ አካል ነው።...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18751</th>\n",
       "      <td>ተችሏል ተብሏል። በሌላ ዜና በአሸባሪው ሸኔ አባላት መካከል የተፈጠረው አ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38337</th>\n",
       "      <td>ያጋልጠዋል። ለ39 አመታት ኮማ ውስጥ ከቆየ በኋላ ዛሬ በ73 አመቱ ከዚህ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>ሹመቶችን ካገኘው ከተቃዋሚው ብሄራዊ ባይቶና አባይ ትግራይ ፓርቲ እንደኾነ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37065</th>\n",
       "      <td>አስተማማኝ ዝውውር አደረገ ። || ማን ሲቲ በቀጣዩ ክረምት ሜሲን ለማስፈ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10796</th>\n",
       "      <td>ዋና አሰልጣኝ)፡ በጣም የተወዳደርን ይመስለኛል፣ እዚህ በጣም ደስተኛ ነኝ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11007</th>\n",
       "      <td>እንደማይዝ በማወቁ ነው ጥፋቱን የፈፀመው ሮድሪጎ ያችን ኳስ በፍጥነት ባሄ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14322</th>\n",
       "      <td>ተንቀሳቅሰዋል። ቡድኑ በቅድሚያ በጅግጅጋ ከተማ ቅዳሜ ሚያዝያ 29 ቀን 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36493</th>\n",
       "      <td>አሜሪካዊ አርቲስት 1 Photo Instagram ላይ በለቀቀች ቁጥር 12 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41271</th>\n",
       "      <td>ኒውካስትል ማርክ ኩኮሬላ እየተከታተሉት ነው! Mirror ሰበር ዜና! የሳ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "47104  እንፈልጋለን ፤ አጨራረሱን እንፈልጋለን ኦባ የስኬታችን አንዱ አካል ነው።...\n",
       "18751  ተችሏል ተብሏል። በሌላ ዜና በአሸባሪው ሸኔ አባላት መካከል የተፈጠረው አ...\n",
       "38337  ያጋልጠዋል። ለ39 አመታት ኮማ ውስጥ ከቆየ በኋላ ዛሬ በ73 አመቱ ከዚህ...\n",
       "1973   ሹመቶችን ካገኘው ከተቃዋሚው ብሄራዊ ባይቶና አባይ ትግራይ ፓርቲ እንደኾነ...\n",
       "37065  አስተማማኝ ዝውውር አደረገ ። || ማን ሲቲ በቀጣዩ ክረምት ሜሲን ለማስፈ...\n",
       "...                                                  ...\n",
       "10796  ዋና አሰልጣኝ)፡ በጣም የተወዳደርን ይመስለኛል፣ እዚህ በጣም ደስተኛ ነኝ...\n",
       "11007  እንደማይዝ በማወቁ ነው ጥፋቱን የፈፀመው ሮድሪጎ ያችን ኳስ በፍጥነት ባሄ...\n",
       "14322  ተንቀሳቅሰዋል። ቡድኑ በቅድሚያ በጅግጅጋ ከተማ ቅዳሜ ሚያዝያ 29 ቀን 2...\n",
       "36493  አሜሪካዊ አርቲስት 1 Photo Instagram ላይ በለቀቀች ቁጥር 12 ...\n",
       "41271  ኒውካስትል ማርክ ኩኮሬላ እየተከታተሉት ነው! Mirror ሰበር ዜና! የሳ...\n",
       "\n",
       "[4898 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Divide the dataset into train and test categories \n",
    "msk = np.random.rand(len(dataset_2)) < 0.8\n",
    "train_dataset = dataset_2[msk]\n",
    "test_dataset = dataset_2[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
    "from datasets import Dataset\n",
    "\n",
    "test_dataset=Dataset.from_pandas(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
    "train_dataset=Dataset.from_pandas(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
    "evaluation_dataset=Dataset.from_pandas(evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', '__index_level_0__'],\n",
       "    num_rows: 12404\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove unnecessary column\n",
    "test_dataset=test_dataset.remove_columns(\"__index_level_0__\")\n",
    "train_dataset=train_dataset.remove_columns(\"__index_level_0__\")\n",
    "evaluation_dataset=evaluation_dataset.remove_columns(\"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datasets\n",
    "#combine the train and test dataset into one datset\n",
    "main_dataset= datasets.DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'evaluate': evaluation_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text'],\n",
       "        num_rows: 48820\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text'],\n",
       "        num_rows: 12404\n",
       "    })\n",
       "    evaluate: Dataset({\n",
       "        features: ['Text'],\n",
       "        num_rows: 4898\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"NousResearch/Llama-2-7b-hf\"\n",
    "new_model = \"llama-2-7b-Amh\"\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{23000}MB'\n",
    "\n",
    "load_model(base_model,bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b6ac9f41984618914d14fe3ba35a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load base moodel\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset=main_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4273 MiB |   4523 MiB |  17541 MiB |  13267 MiB |\n",
      "|       from large pool |   4110 MiB |   4360 MiB |  17247 MiB |  13137 MiB |\n",
      "|       from small pool |    163 MiB |    163 MiB |    293 MiB |    130 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4273 MiB |   4523 MiB |  17541 MiB |  13267 MiB |\n",
      "|       from large pool |   4110 MiB |   4360 MiB |  17247 MiB |  13137 MiB |\n",
      "|       from small pool |    163 MiB |    163 MiB |    293 MiB |    130 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4251 MiB |   4501 MiB |  17490 MiB |  13239 MiB |\n",
      "|       from large pool |   4088 MiB |   4338 MiB |  17198 MiB |  13110 MiB |\n",
      "|       from small pool |    163 MiB |    163 MiB |    292 MiB |    129 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   4468 MiB |   4968 MiB |   4970 MiB | 514048 KiB |\n",
      "|       from large pool |   4300 MiB |   4800 MiB |   4800 MiB | 512000 KiB |\n",
      "|       from small pool |    168 MiB |    168 MiB |    170 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 198879 KiB | 202003 KiB |   7826 MiB |   7632 MiB |\n",
      "|       from large pool | 194048 KiB | 194560 KiB |   7595 MiB |   7405 MiB |\n",
      "|       from small pool |   4831 KiB |   7860 KiB |    231 MiB |    226 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1284    |    1508    |    3591    |    2307    |\n",
      "|       from large pool |     226    |     227    |     548    |     322    |\n",
      "|       from small pool |    1058    |    1282    |    3043    |    1985    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1284    |    1508    |    3591    |    2307    |\n",
      "|       from large pool |     226    |     227    |     548    |     322    |\n",
      "|       from small pool |    1058    |    1282    |    3043    |    1985    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     143    |     145    |     146    |       3    |\n",
      "|       from large pool |      59    |      61    |      61    |       2    |\n",
      "|       from small pool |      84    |      84    |      85    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     138    |     138    |     649    |     511    |\n",
      "|       from large pool |      44    |      44    |     251    |     207    |\n",
      "|       from small pool |      94    |      94    |     398    |     304    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Use the appropriate device (cuda:0, cuda:1, etc.)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Empty the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print GPU memory usage\n",
    "print(torch.cuda.memory_summary(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: Total memory: 22.01849365234375 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check the number of available GPUs\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {n_gpus}\")\n",
    "\n",
    "# Check the memory of each available GPU\n",
    "for i in range(n_gpus):\n",
    "    gpu_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "    print(f\"GPU {i}: Total memory: {gpu_memory / (1024**3)} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8511605d6a96400da7197404fbec968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ee662aa92047b1bc3dd74c0b8100bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melias-assamnew\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/elias_assamnew/week_7_llm_finetuning_for_amharic_language/backend/notebooks/wandb/run-20240202_025849-zqz9fvg7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elias-assamnew/huggingface/runs/zqz9fvg7' target=\"_blank\">logical-shadow-9</a></strong> to <a href='https://wandb.ai/elias-assamnew/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elias-assamnew/huggingface' target=\"_blank\">https://wandb.ai/elias-assamnew/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elias-assamnew/huggingface/runs/zqz9fvg7' target=\"_blank\">https://wandb.ai/elias-assamnew/huggingface/runs/zqz9fvg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"../results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=10,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=10,\n",
    "        # report_to=\"wandb\",\n",
    "        max_steps=10, # Remove this line for a real fine-tuning\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=main_dataset[\"train\"],\n",
    "    eval_dataset=main_dataset[\"evaluate\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "# data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "የኢትዮጽያ ጂዲፒ አስተዋይን አለም\n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our model\n",
    "prompt = \"የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "በአማርኛ በማለት በመጀመሪያ መጀመሪያ በመጀመሪያ በመጀመሪያ\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our model\n",
    "prompt = \"how can i treat flu, give the response in amharic language\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<p>አስተዋጭ አስተዋጭ አስተዋጭ አስተዋጭ አስተዋጭ አስተዋጭ\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our model\n",
    "prompt = \"tell me about ethiopian politics, give the response in amharic language\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<p>አስተዳደር አለም አለም አለም አለም አለም አለም አለም አለ\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our model\n",
    "prompt = \"who the prime minister of ethiopia, give the response in amharic language\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<p>በአማርኛ በማለት የአማርኛ ተራራ መግለጫ በአማርኛ ተራራ ��\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our model\n",
    "prompt = \"3 Ethiopian premier league club, give the response in amharic language\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the base model with the trained Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772bab9276184dd5834cc7394ee7b0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 22.02 GiB of which 2.25 MiB is free. Process 31451 has 5.34 GiB memory in use. Including non-PyTorch memory, this process has 16.66 GiB memory in use. Of the allocated memory 15.47 GiB is allocated by PyTorch, and 159.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reload model in FP16 and merge it with LoRA weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m     base_model,\n\u001b[1;32m      4\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      7\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, new_model)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m~/.conda/envs/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:567\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    566\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    568\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3856\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3847\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3848\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3849\u001b[0m     (\n\u001b[1;32m   3850\u001b[0m         model,\n\u001b[1;32m   3851\u001b[0m         missing_keys,\n\u001b[1;32m   3852\u001b[0m         unexpected_keys,\n\u001b[1;32m   3853\u001b[0m         mismatched_keys,\n\u001b[1;32m   3854\u001b[0m         offload_index,\n\u001b[1;32m   3855\u001b[0m         error_msgs,\n\u001b[0;32m-> 3856\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3857\u001b[0m         model,\n\u001b[1;32m   3858\u001b[0m         state_dict,\n\u001b[1;32m   3859\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3860\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3861\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3862\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3863\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3864\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3865\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3866\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3867\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3868\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3869\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3870\u001b[0m         is_quantized\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES),\n\u001b[1;32m   3871\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3872\u001b[0m     )\n\u001b[1;32m   3874\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3875\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/.conda/envs/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4290\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4286\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4287\u001b[0m                         model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4288\u001b[0m                     )\n\u001b[1;32m   4289\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4290\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4291\u001b[0m             model_to_load,\n\u001b[1;32m   4292\u001b[0m             state_dict,\n\u001b[1;32m   4293\u001b[0m             loaded_keys,\n\u001b[1;32m   4294\u001b[0m             start_prefix,\n\u001b[1;32m   4295\u001b[0m             expected_keys,\n\u001b[1;32m   4296\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4297\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4298\u001b[0m             offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[1;32m   4299\u001b[0m             state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[1;32m   4300\u001b[0m             state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[1;32m   4301\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   4302\u001b[0m             is_quantized\u001b[38;5;241m=\u001b[39mis_quantized,\n\u001b[1;32m   4303\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[1;32m   4304\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4305\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4306\u001b[0m         )\n\u001b[1;32m   4307\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:805\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    802\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized:\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mint8, torch\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;129;01mand\u001b[39;00m is_quantized:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# handling newly quantized weights and loaded quantized weights\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# edit the param.dtype restrictions and is_quantized condition when adding new quant methods\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     quantized_stats \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.conda/envs/venv/lib/python3.11/site-packages/accelerate/utils/modeling.py:379\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    377\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 379\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 22.02 GiB of which 2.25 MiB is free. Process 31451 has 5.34 GiB memory in use. Including non-PyTorch memory, this process has 16.66 GiB memory in use. Of the allocated memory 15.47 GiB is allocated by PyTorch, and 159.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
