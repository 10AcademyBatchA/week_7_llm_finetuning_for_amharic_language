{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face token\n",
    "hf_token = os.getenv(\"hf_token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "import peft\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1Which one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1 C feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democracy The president leads the Head of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are educated guessHowever prediction must be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61219</th>\n",
       "      <td>አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61220</th>\n",
       "      <td>ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61221</th>\n",
       "      <td>አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61222</th>\n",
       "      <td>በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61223</th>\n",
       "      <td>ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61224 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      Civics Unit 1 Grade 11 & 12 hybrid 1Which one ...\n",
       "1      Civics Unit 1 Grade 11 & 12 hybrid 1 C feature...\n",
       "2      democracy The president leads the Head of the ...\n",
       "3      አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...\n",
       "4      are educated guessHowever prediction must be t...\n",
       "...                                                  ...\n",
       "61219  አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...\n",
       "61220  ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...\n",
       "61221  አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...\n",
       "61222  በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...\n",
       "61223  ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...\n",
       "\n",
       "[61224 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '../../merged.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1Which one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1 C feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democracy The president leads the Head of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are educated guessHowever prediction must be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61219</th>\n",
       "      <td>አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61220</th>\n",
       "      <td>ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61221</th>\n",
       "      <td>አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61222</th>\n",
       "      <td>በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61223</th>\n",
       "      <td>ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61224 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      Civics Unit 1 Grade 11 & 12 hybrid 1Which one ...\n",
       "1      Civics Unit 1 Grade 11 & 12 hybrid 1 C feature...\n",
       "2      democracy The president leads the Head of the ...\n",
       "3      አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...\n",
       "4      are educated guessHowever prediction must be t...\n",
       "...                                                  ...\n",
       "61219  አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...\n",
       "61220  ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...\n",
       "61221  አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...\n",
       "61222  በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...\n",
       "61223  ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...\n",
       "\n",
       "[61224 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=df[['Text']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2=dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# # Create a dictionary containing your Amharic text data\n",
    "# data_dict = {\"Text\": dataset['Text'].tolist()}\n",
    "\n",
    "# # Create a Dataset object\n",
    "# dataset_2 = Dataset.from_dict(data_dict)\n",
    "# dataset_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1Which one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Civics Unit 1 Grade 11 &amp; 12 hybrid 1 C feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democracy The president leads the Head of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are educated guessHowever prediction must be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61219</th>\n",
       "      <td>አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61220</th>\n",
       "      <td>ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61221</th>\n",
       "      <td>አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61222</th>\n",
       "      <td>በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61223</th>\n",
       "      <td>ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61224 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      Civics Unit 1 Grade 11 & 12 hybrid 1Which one ...\n",
       "1      Civics Unit 1 Grade 11 & 12 hybrid 1 C feature...\n",
       "2      democracy The president leads the Head of the ...\n",
       "3      አልሰራ ላላችሁ እዚሁ ስላወረድንላችሁ ስልካችሁ ላይ ጭናችሁ መጠቀም ትችላ...\n",
       "4      are educated guessHowever prediction must be t...\n",
       "...                                                  ...\n",
       "61219  አደረሳችሁ አደረሰን! ክህደትን ከሁሉም የከፋ የሚያደርገው ከጠላት የማይመ...\n",
       "61220  ትወድሃለች ወንድ ልጅ ባንቺ ምክንያት ፈገግ ካለ ይወድሻል ተሰብስበው አሙ...\n",
       "61221  አሁንም ድረስ ልቤ ይደነግጥልሻል ልክ እንደ መጀመሪያው። የቅዱስ ላሊበላ ...\n",
       "61222  በተግባሬ መረዳት ትችያለሽ። ለእያንዳንዱ ስሜቴ ቃላቶቼን አውጥቼ እንዲህ ...\n",
       "61223  ነው አነሱ ደግሞ ከሌላ ሰው ሊከብድ ይችላል ግን ተስፋውን ተውና እውነቱን...\n",
       "\n",
       "[61224 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (1.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: 44081\n",
      "evaluation dataset shape: 4898\n",
      "Testing dataset shape: 12245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_val_data, test_data = train_test_split(dataset_2, test_size=0.20, random_state=42)\n",
    "train_data, evaluation_data = train_test_split(train_val_data, test_size=0.10, random_state=42)\n",
    "\n",
    "print('Training dataset shape:', len(train_data))\n",
    "print('evaluation dataset shape:', len(evaluation_data))\n",
    "print('Testing dataset shape:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47104</th>\n",
       "      <td>እንፈልጋለን ፤ አጨራረሱን እንፈልጋለን ኦባ የስኬታችን አንዱ አካል ነው።...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18751</th>\n",
       "      <td>ተችሏል ተብሏል። በሌላ ዜና በአሸባሪው ሸኔ አባላት መካከል የተፈጠረው አ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38337</th>\n",
       "      <td>ያጋልጠዋል። ለ39 አመታት ኮማ ውስጥ ከቆየ በኋላ ዛሬ በ73 አመቱ ከዚህ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>ሹመቶችን ካገኘው ከተቃዋሚው ብሄራዊ ባይቶና አባይ ትግራይ ፓርቲ እንደኾነ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37065</th>\n",
       "      <td>አስተማማኝ ዝውውር አደረገ ። || ማን ሲቲ በቀጣዩ ክረምት ሜሲን ለማስፈ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10796</th>\n",
       "      <td>ዋና አሰልጣኝ)፡ በጣም የተወዳደርን ይመስለኛል፣ እዚህ በጣም ደስተኛ ነኝ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11007</th>\n",
       "      <td>እንደማይዝ በማወቁ ነው ጥፋቱን የፈፀመው ሮድሪጎ ያችን ኳስ በፍጥነት ባሄ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14322</th>\n",
       "      <td>ተንቀሳቅሰዋል። ቡድኑ በቅድሚያ በጅግጅጋ ከተማ ቅዳሜ ሚያዝያ 29 ቀን 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36493</th>\n",
       "      <td>አሜሪካዊ አርቲስት 1 Photo Instagram ላይ በለቀቀች ቁጥር 12 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41271</th>\n",
       "      <td>ኒውካስትል ማርክ ኩኮሬላ እየተከታተሉት ነው! Mirror ሰበር ዜና! የሳ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "47104  እንፈልጋለን ፤ አጨራረሱን እንፈልጋለን ኦባ የስኬታችን አንዱ አካል ነው።...\n",
       "18751  ተችሏል ተብሏል። በሌላ ዜና በአሸባሪው ሸኔ አባላት መካከል የተፈጠረው አ...\n",
       "38337  ያጋልጠዋል። ለ39 አመታት ኮማ ውስጥ ከቆየ በኋላ ዛሬ በ73 አመቱ ከዚህ...\n",
       "1973   ሹመቶችን ካገኘው ከተቃዋሚው ብሄራዊ ባይቶና አባይ ትግራይ ፓርቲ እንደኾነ...\n",
       "37065  አስተማማኝ ዝውውር አደረገ ። || ማን ሲቲ በቀጣዩ ክረምት ሜሲን ለማስፈ...\n",
       "...                                                  ...\n",
       "10796  ዋና አሰልጣኝ)፡ በጣም የተወዳደርን ይመስለኛል፣ እዚህ በጣም ደስተኛ ነኝ...\n",
       "11007  እንደማይዝ በማወቁ ነው ጥፋቱን የፈፀመው ሮድሪጎ ያችን ኳስ በፍጥነት ባሄ...\n",
       "14322  ተንቀሳቅሰዋል። ቡድኑ በቅድሚያ በጅግጅጋ ከተማ ቅዳሜ ሚያዝያ 29 ቀን 2...\n",
       "36493  አሜሪካዊ አርቲስት 1 Photo Instagram ላይ በለቀቀች ቁጥር 12 ...\n",
       "41271  ኒውካስትል ማርክ ኩኮሬላ እየተከታተሉት ነው! Mirror ሰበር ዜና! የሳ...\n",
       "\n",
       "[4898 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Divide the dataset into train and test categories \n",
    "msk = np.random.rand(len(dataset_2)) < 0.8\n",
    "train_dataset = dataset_2[msk]\n",
    "test_dataset = dataset_2[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
    "from datasets import Dataset\n",
    "\n",
    "test_dataset=Dataset.from_pandas(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
    "train_dataset=Dataset.from_pandas(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
    "evaluation_dataset=Dataset.from_pandas(evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', '__index_level_0__'],\n",
       "    num_rows: 12267\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove unnecessary column\n",
    "test_dataset=test_dataset.remove_columns(\"__index_level_0__\")\n",
    "train_dataset=train_dataset.remove_columns(\"__index_level_0__\")\n",
    "evaluation_dataset=evaluation_dataset.remove_columns(\"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datasets\n",
    "#combine the train and test dataset into one datset\n",
    "main_dataset= datasets.DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'evaluate': evaluation_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text'],\n",
       "        num_rows: 48957\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text'],\n",
       "        num_rows: 12267\n",
       "    })\n",
       "    evaluate: Dataset({\n",
       "        features: ['Text'],\n",
       "        num_rows: 4898\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"NousResearch/Llama-2-7b-hf\"\n",
    "new_model = \"llama-2-7b-Amh\"\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{23000}MB'\n",
    "\n",
    "load_model(base_model,bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'allocated_bytes.all.current'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print GPU memory usage\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_summary(device))\n",
      "File \u001b[0;32m~/.conda/envs/venv/lib/python3.11/site-packages/torch/cuda/memory.py:557\u001b[0m, in \u001b[0;36mmemory_summary\u001b[0;34m(device, abbreviated)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submetric_key, submetric_name \u001b[38;5;129;01min\u001b[39;00m submetrics:\n\u001b[1;32m    555\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m metric_key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m submetric_key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 557\u001b[0m     current \u001b[38;5;241m=\u001b[39m stats[prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    558\u001b[0m     peak \u001b[38;5;241m=\u001b[39m stats[prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    559\u001b[0m     allocated \u001b[38;5;241m=\u001b[39m stats[prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallocated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'allocated_bytes.all.current'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Use the appropriate device (cuda:0, cuda:1, etc.)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Empty the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print GPU memory usage\n",
    "print(torch.cuda.memory_summary(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607b813b8cda410b9168987c0aac642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load base moodel\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset=main_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4273 MiB |   4523 MiB |  17541 MiB |  13267 MiB |\n",
      "|       from large pool |   4110 MiB |   4360 MiB |  17247 MiB |  13137 MiB |\n",
      "|       from small pool |    163 MiB |    163 MiB |    293 MiB |    130 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4273 MiB |   4523 MiB |  17541 MiB |  13267 MiB |\n",
      "|       from large pool |   4110 MiB |   4360 MiB |  17247 MiB |  13137 MiB |\n",
      "|       from small pool |    163 MiB |    163 MiB |    293 MiB |    130 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4251 MiB |   4501 MiB |  17490 MiB |  13239 MiB |\n",
      "|       from large pool |   4088 MiB |   4338 MiB |  17198 MiB |  13110 MiB |\n",
      "|       from small pool |    163 MiB |    163 MiB |    292 MiB |    129 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   4468 MiB |   4968 MiB |   4970 MiB | 514048 KiB |\n",
      "|       from large pool |   4300 MiB |   4800 MiB |   4800 MiB | 512000 KiB |\n",
      "|       from small pool |    168 MiB |    168 MiB |    170 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 198879 KiB | 202003 KiB |   7826 MiB |   7632 MiB |\n",
      "|       from large pool | 194048 KiB | 194560 KiB |   7595 MiB |   7405 MiB |\n",
      "|       from small pool |   4831 KiB |   7860 KiB |    231 MiB |    226 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1284    |    1508    |    3591    |    2307    |\n",
      "|       from large pool |     226    |     227    |     548    |     322    |\n",
      "|       from small pool |    1058    |    1282    |    3043    |    1985    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1284    |    1508    |    3591    |    2307    |\n",
      "|       from large pool |     226    |     227    |     548    |     322    |\n",
      "|       from small pool |    1058    |    1282    |    3043    |    1985    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     143    |     145    |     146    |       3    |\n",
      "|       from large pool |      59    |      61    |      61    |       2    |\n",
      "|       from small pool |      84    |      84    |      85    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     138    |     138    |     649    |     511    |\n",
      "|       from large pool |      44    |      44    |     251    |     207    |\n",
      "|       from small pool |      94    |      94    |     398    |     304    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Use the appropriate device (cuda:0, cuda:1, etc.)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Empty the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print GPU memory usage\n",
    "print(torch.cuda.memory_summary(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: Total memory: 22.01849365234375 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check the number of available GPUs\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {n_gpus}\")\n",
    "\n",
    "# Check the memory of each available GPU\n",
    "for i in range(n_gpus):\n",
    "    gpu_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "    print(f\"GPU {i}: Total memory: {gpu_memory / (1024**3)} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729b32042d614957a8464c4ea619bbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48957 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melias-assamnew\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/elias_assamnew/week_7_llm_finetuning_for_amharic_language/backend/notebooks/wandb/run-20240202_015752-nmynhlad</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elias-assamnew/huggingface/runs/nmynhlad' target=\"_blank\">avid-lake-8</a></strong> to <a href='https://wandb.ai/elias-assamnew/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elias-assamnew/huggingface' target=\"_blank\">https://wandb.ai/elias-assamnew/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elias-assamnew/huggingface/runs/nmynhlad' target=\"_blank\">https://wandb.ai/elias-assamnew/huggingface/runs/nmynhlad</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias_assamnew/.conda/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"../results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        # per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=1,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=2,\n",
    "        # report_to=\"wandb\",\n",
    "        max_steps=10, # Remove this line for a real fine-tuning\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=main_dataset[\"train\"],\n",
    "    # eval_dataset=main_dataset[\"evaluate\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "# data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stream *without* function calling capabilities\n",
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = 'You are a helpful assistant that provides accurate and concise responses in Amharic'\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream('eziga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mየኢትዮጽያ  ጂዲፒ ምን ያህል ነበር\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Instruction:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m pipe(instruction)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mlen\u001b[39m(instruction):])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our model\n",
    "prompt = \"የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer)\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mየኢትዮጽያ  ጂዲፒ ምን ያህል ነበር\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "\n",
    "prompt = \"የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር\"\n",
    "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(instruction)\n",
    "print(result[0]['generated_text'][len(instruction):])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
