{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flagembedding 1.1.8 requires transformers==4.34.0, but you have transformers 4.36.2 which is incompatible.\n",
      "ragas 0.0.22 requires openai>1, but you have openai 0.28.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain==0.0.292 \\\n",
    "#     openai==0.28.0 \\\n",
    "#     datasets==2.10.1 \\\n",
    "#     pinecone-client==2.2.4 \\\n",
    "#     tiktoken==0.5.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Exploration for Amharic Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY= os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "# print(OPENAI_API_KEY) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three _message_ objects like so:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response we get another AI message object. We can print it more clearly like so:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ኢትዮጵያ ታሪክ ከአንድ ዓመት በላይ ነው የሚጠራው?')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. we make Amharic questions and you make a brief answers in amharic language \"),\n",
    "    HumanMessage(content=\"ሃይ ሰላም ነው??\"),\n",
    "    AIMessage(content=\"እግዚአብሔር ይመስገን ሰላም ነኝ? ምን ልርዳዎ??\"),\n",
    "    HumanMessage(content=\"ሰለ ኢትዮጵያ ታሪክ ንገረኝ\")\n",
    "]\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='እግዚአብሔር ይመስገን ሰላም ነኝ።')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. we make Amharic questions and you make a brief answers in amharic language \"),\n",
    "    HumanMessage(content=\"ሃይ ሰላም ነው?\"),\n",
    "    AIMessage(content=\"እግዚአብሔር ይመስገን ሰላም ነኝ? ምን ልርዳዎ??\"),\n",
    "    HumanMessage(content=\"ሃይ ሰላም ነው?\")\n",
    "]\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='እግዚአብሔር ከአንተ ጋር ነው (ሴላም 23:4)\\nእግዚአብሔር መልካም ነው (መጽሐፍ ቅዱስ ዮሐንስ 11:28)')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. we make Amharic questions and you make a brief answers in amharic language \"),\n",
    "    HumanMessage(content=\"ሃይ ሰላም ነው?\"),\n",
    "    AIMessage(content=\"እግዚአብሔር ይመስገን ሰላም ነኝ? ምን ልርዳዎ??\"),\n",
    "    HumanMessage(content=\"tell me two bible verse in amharic?\")\n",
    "    ]\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ሁሉም ኢትዮጵያውያንን የሰላም አሰራር አድራሻ ያለው ስፕራንት ቡጥ ሊገኝ እንደሚችል ይጠቀሙ።')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. we make Amharic questions and you make a brief answers in amharic language \"),\n",
    "    HumanMessage(content=\"ሃይ ሰላም ነው?\"),\n",
    "    AIMessage(content=\"እግዚአብሔር ይመስገን ሰላም ነኝ? ምን ልርዳዎ??\"),\n",
    "    HumanMessage(content=\"tell me two ethiopian premier league club  in amharic?\")\n",
    "    ]\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ሁሉም ሰው በሁሉም በኩል እንዲሁም በሁሉም በኩል በመጠበቅ ለመስራት የሚደረግ የህብረት ምልክት\\nበሰላምና በጥርስ አለው።')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. we make Amharic questions and you make a brief answers in amharic language \"),\n",
    "    HumanMessage(content=\"ሃይ ሰላም ነው?\"),\n",
    "    AIMessage(content=\"እግዚአብሔር ይመስገን ሰላም ነኝ? ምን ልርዳዎ??\"),\n",
    "    HumanMessage(content=\"tell me two humanitarian law  in amharic?\")\n",
    "    ]\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='የኢትዮጵያ መሪ አለማየሁሽ ብዙ ሰዎች የሚታየውን ማንም አይነቱን ሊያውቅ ይችላል። አንድነት አይደለም።')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. we make Amharic questions and you make a brief answers in amharic language \"),\n",
    "    HumanMessage(content=\"ሃይ ሰላም ነው?\"),\n",
    "    AIMessage(content=\"እግዚአብሔር ይመስገን ሰላም ነኝ? ምን ልርዳዎ??\"),\n",
    "    HumanMessage(content=\"የኢትዮጵያ መሪ ማን ነው??\")\n",
    "    ]\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "amh_information = [\n",
    "    \"ሐምሌ 22 ቀን 1928 ስምንት ወታደሮች በስተጀርባቸው ሃያ እርምጃ ርቀው ተንበርክከው በተንጠቀቅ ቆሙ።\\nወዲያው አዛዡ «ተኩስ» በማለት ትእዛዝ ሲሰጥ ስምንቱም ተኩሰው መቷቸው። ግን በስምንት ጥይት ሳይሞቱ ቀሩ። መሞታቸውንና አለመሞታቸውን ለማረጋገጥ ዶክተር ተጠራ። ዶክተሩም እንዳልሞቱ አረጋገጠ። ከዚያም ሌላ ወታደር በሦስት የሽጉጥ ጥይት ራስቅላቸውን መትቶ ገደላቸው።\\n\\nአቡነ ጴጥሮስ ለእናት ሀገራቸው ኢትዮጵያ ክብር ሲሉ የሞትን ፅዋ ተጎነጩ ።\\n\\nለዛሬ እኛነታችን ያለፉት ትውልዶች ብዙ ዋጋ ከፍለዋል እና ሊዘከሩ፣ሊመሰገኑና ሊከበሩ ይገባል። የኃላው ከሌለ የለም የፊቱ ! \"\n",
    "\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(amh_information)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ሐምሌ 22 ምን ተፈጠረ??\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ሐምሌ 22 ምን ተፈጠረ? ተፈጠረው የኢትዮጵያ መሪ ነው።\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
